{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import AveragePooling2D, Dropout, Flatten, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/PIL/Image.py:918: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n"
     ]
    }
   ],
   "source": [
    "directory = r\"dataset\"\n",
    "categories = [\"with_mask\", \"without_mask\"]\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "#reading images from the dateset directory\n",
    "for category in categories:\n",
    "    path = os.path.join(directory,category)\n",
    "    for image in os.listdir(path):\n",
    "        img_path = os.path.join(path, image)\n",
    "        image = load_img(img_path, target_size = (224,224))\n",
    "        image = img_to_array(image)\n",
    "        image = preprocess_input(image)\n",
    "        data.append(image)\n",
    "        labels.append(category)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encoding on the labels\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "data = np.array(data, dtype = \"float32\")\n",
    "labels = np.array(labels)\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(data, labels, test_size=0.20, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#augmentating the data\n",
    "aug = ImageDataGenerator(rotation_range = 20,\n",
    "                         zoom_range = 0.15,\n",
    "                         width_shift_range = 0.2,\n",
    "                         height_shift_range = 0.2,\n",
    "                         shear_range = 0.15,\n",
    "                         horizontal_flip = True,\n",
    "                         fill_mode = \"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9412608/9406464 [==============================] - 36s 4us/step\n"
     ]
    }
   ],
   "source": [
    "# load the MobileNetV2 network, ensuring the head FC layer sets are off\n",
    "basemodel = MobileNetV2(weights = \"imagenet\", include_top = False, input_tensor = Input(shape = (224,224,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the head of the model that will be placed on top of the base model\n",
    "headmodel = basemodel.output\n",
    "headmodel = AveragePooling2D(pool_size = (7, 7))(headmodel)\n",
    "headmodel = Flatten(name = 'flatten')(headmodel)\n",
    "headmodel = Dense(128, activation = 'relu')(headmodel)\n",
    "headmodel = Dense(2, activation = 'softmax')(headmodel)\n",
    "\n",
    "# place the head FC model on top of the base model (this will become the actual model we will train)\n",
    "model = Model(inputs = basemodel.input, outputs = headmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze all the basemodel layers so they will not be updated during training\n",
    "for layer in basemodel.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr = 1e-4, decay = 1e-4/20)\n",
    "#compiling model\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = optimizer, metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "95/95 [==============================] - 60s 633ms/step - loss: 0.2333 - accuracy: 0.9038 - val_loss: 0.0798 - val_accuracy: 0.9791\n",
      "Epoch 2/20\n",
      "95/95 [==============================] - 66s 691ms/step - loss: 0.0632 - accuracy: 0.9825 - val_loss: 0.0491 - val_accuracy: 0.9857\n",
      "Epoch 3/20\n",
      "95/95 [==============================] - 64s 676ms/step - loss: 0.0483 - accuracy: 0.9842 - val_loss: 0.0414 - val_accuracy: 0.9870\n",
      "Epoch 4/20\n",
      "95/95 [==============================] - 61s 643ms/step - loss: 0.0416 - accuracy: 0.9878 - val_loss: 0.0421 - val_accuracy: 0.9870\n",
      "Epoch 5/20\n",
      "95/95 [==============================] - 61s 641ms/step - loss: 0.0290 - accuracy: 0.9914 - val_loss: 0.0352 - val_accuracy: 0.9883\n",
      "Epoch 6/20\n",
      "95/95 [==============================] - 61s 639ms/step - loss: 0.0305 - accuracy: 0.9885 - val_loss: 0.0278 - val_accuracy: 0.9935\n",
      "Epoch 7/20\n",
      "95/95 [==============================] - 61s 641ms/step - loss: 0.0288 - accuracy: 0.9904 - val_loss: 0.0277 - val_accuracy: 0.9922\n",
      "Epoch 8/20\n",
      "95/95 [==============================] - 61s 643ms/step - loss: 0.0231 - accuracy: 0.9921 - val_loss: 0.0337 - val_accuracy: 0.9870\n",
      "Epoch 9/20\n",
      "95/95 [==============================] - 60s 637ms/step - loss: 0.0167 - accuracy: 0.9957 - val_loss: 0.0217 - val_accuracy: 0.9974\n",
      "Epoch 10/20\n",
      "95/95 [==============================] - 61s 639ms/step - loss: 0.0180 - accuracy: 0.9934 - val_loss: 0.0285 - val_accuracy: 0.9883\n",
      "Epoch 11/20\n",
      "95/95 [==============================] - 60s 634ms/step - loss: 0.0201 - accuracy: 0.9934 - val_loss: 0.0245 - val_accuracy: 0.9961\n",
      "Epoch 12/20\n",
      "95/95 [==============================] - 61s 638ms/step - loss: 0.0161 - accuracy: 0.9937 - val_loss: 0.0192 - val_accuracy: 0.9961\n",
      "Epoch 13/20\n",
      "95/95 [==============================] - 61s 640ms/step - loss: 0.0136 - accuracy: 0.9964 - val_loss: 0.0263 - val_accuracy: 0.9909\n",
      "Epoch 14/20\n",
      "95/95 [==============================] - 60s 635ms/step - loss: 0.0142 - accuracy: 0.9967 - val_loss: 0.0216 - val_accuracy: 0.9948\n",
      "Epoch 15/20\n",
      "95/95 [==============================] - 61s 640ms/step - loss: 0.0123 - accuracy: 0.9960 - val_loss: 0.0201 - val_accuracy: 0.9961\n",
      "Epoch 16/20\n",
      "95/95 [==============================] - 61s 639ms/step - loss: 0.0089 - accuracy: 0.9970 - val_loss: 0.0308 - val_accuracy: 0.9857\n",
      "Epoch 17/20\n",
      "95/95 [==============================] - 61s 638ms/step - loss: 0.0144 - accuracy: 0.9951 - val_loss: 0.0206 - val_accuracy: 0.9974\n",
      "Epoch 18/20\n",
      "95/95 [==============================] - 62s 651ms/step - loss: 0.0125 - accuracy: 0.9967 - val_loss: 0.0195 - val_accuracy: 0.9974\n",
      "Epoch 19/20\n",
      "95/95 [==============================] - 65s 689ms/step - loss: 0.0101 - accuracy: 0.9967 - val_loss: 0.0189 - val_accuracy: 0.9974\n",
      "Epoch 20/20\n",
      "95/95 [==============================] - 63s 663ms/step - loss: 0.0084 - accuracy: 0.9977 - val_loss: 0.0237 - val_accuracy: 0.9896\n"
     ]
    }
   ],
   "source": [
    "#training headmodel\n",
    "trained_model = model.fit(aug.flow(trainX, trainY, batch_size = 32),\n",
    "                         steps_per_epoch = len(trainX)//32,\n",
    "                         validation_data = (testX, testY),\n",
    "                         validation_steps = len(testX)//32,\n",
    "                         epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction\n",
    "predict = model.predict(testX, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   with_mask       0.98      1.00      0.99       383\n",
      "without_mask       1.00      0.98      0.99       384\n",
      "\n",
      "    accuracy                           0.99       767\n",
      "   macro avg       0.99      0.99      0.99       767\n",
      "weighted avg       0.99      0.99      0.99       767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#based on largeset probability between with mask and without mask we need to classify the images \n",
    "predict = np.argmax(predict, axis = 1)\n",
    "#classificaiton report\n",
    "print(classification_report(testY.argmax(axis = 1), predict, target_names = lb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model to the disk\n",
    "model.save(\"mask_detector.model\", save_format=\"h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
